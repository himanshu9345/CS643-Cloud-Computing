#########################################
INFO ABOUT AMI CS643HadoopImage_Patel
- It is loaded with openjdk-8-jdk
- Hadoop(Hadoop 3.2.1)is configured
- All environment variables are set
#########################################
Steps in creating AMI CS643HadoopImage_Patel
1. Lauch AWS ec2 t2.medium instance with Amamzon linux AMI.
2. Install OpenJDK Java
sudo apt-get update && sudo apt-get dist-upgrade
sudo apt-get install openjdk-8-jdk
3. Download latest hadoop
wget http://apache.mirrors.tds.net/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz -P ~/Downloads
4. Extract tar file and move ti to usr/local
sudo tar zxvf ~/Downloads/hadoop-* -C /usr/local
5. Rename the dir
sudo mv /usr/local/hadoop-* /usr/local/hadoop
6. open bashrc file
nano ~/.bashrc
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export PATH=$PATH:$JAVA_HOME/bin
export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:$HADOOP_HOME/bin
export HADOOP_CONF_DIF=/usr/local/hadoop/etc/hadoop
source ~/.bashrc
Created an image of this machine and make it public
#########################################

REMAINING THINGS
- change the ip of name node
- change hadoop configuration files depending on NameNode or DataNode
- Run program from NameNode.
#########################################
Instructions to Launch Cluter
#########################################
Assuming you aready have keypair.pem file.
1. serach for for CS643HadoopImage_Patel(AMI) in public ami while launching an instance
2. Launch 4 ec2 instance t2.medium machines(save cost) with 25 GB of space and security group open for ALL ports(traffic).
3. Wait for 4 instances to start.
4. Decide among 4 which will be namenode and change the Instance name on AWS its easy for you to remember.


#########################################
My configration
Nodes public DNS visible on aws. 
Replace it with your own.
NameNode - ec2-52-15-227-242.us-east-2.compute.amazonaws.com
Datanode1 - ec2-3-15-152-50.us-east-2.compute.amazonaws.com
Datanode2 - ec2-18-221-233-31.us-east-2.compute.amazonaws.com
Datanode3 - ec2-18-216-24-207.us-east-2.compute.amazonaws.com
internal amazon ip - machine host name
<namenode_IP> namenode_hostname
<datanode1_IP> datanode1_hostname
<datanode2_IP> datanode2_hostname
<datanode3_IP> datanode3_hostname

#########################################
Steps in local computer

- Create a ssh config file
	- touch ~/.ssh/config
	- nano ~/.ssh/config
	Host namenode
	#####################
	  HostName ec2-52-15-227-242.us-east-2.compute.amazonaws.com
	  User ec2-user
	  IdentityFile /home/himanshu/cc_project/himanshu_aws_key.pem 
	Host datanode1
	  HostName ec2-3-15-152-50.us-east-2.compute.amazonaws.com
	  User ec2-user
	  IdentityFile /home/himanshu/cc_project/himanshu_aws_key.pem
	Host datanode2
	  HostName ec2-18-221-233-31.us-east-2.compute.amazonaws.com
	  User ec2-user
	  IdentityFile /home/himanshu/cc_project/himanshu_aws_key.pem
	Host datanode3
	  HostName ec2-18-216-24-207.us-east-2.compute.amazonaws.com
	  User ec2-user
	  IdentityFile /home/himanshu/cc_project/himanshu_aws_key.pem
	#####################
- scp the config file on namenode
	- scp ~/.ssh/config namenode:~/.ssh
- scp pem file to connect to datanodes
	- scp /home/himanshu/cc_project/himanshu_aws_key.pem namenode:~/
##########################################
Steps in NameNode

- connect to namenode
	- ssh namenode
	- change the path of pem file in config
	- nano ~/.ssh/config
	#####################
	Host namenode
	  HostName ec2-52-15-227-242.us-east-2.compute.amazonaws.com
	  User ec2-user
	  IdentityFile ~/himanshu_aws_key.pem 
	Host datanode1
	  HostName ec2-3-15-152-50.us-east-2.compute.amazonaws.com
	  User ec2-user
	  IdentityFile ~/himanshu_aws_key.pem
	Host datanode2
	  HostName ec2-18-221-233-31.us-east-2.compute.amazonaws.com
	  User ec2-user
	  IdentityFile ~/himanshu_aws_key.pem
	Host datanode3
	  HostName ec2-18-216-24-207.us-east-2.compute.amazonaws.com
	  User ec2-user
	  IdentityFile ~/himanshu_aws_key.pem
	#####################
	-  chmod 600 .ssh/config
	- ssh-keygen -f ~/.ssh/id_rsa -t rsa -P ""
	- cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
	- ssh datanode1 'cat >> ~/.ssh/authorized_keys' < ~/.ssh/id_rsa.pub
	- ssh datanode2 'cat >> ~/.ssh/authorized_keys' < ~/.ssh/id_rsa.pub
	- ssh datanode3 'cat >> ~/.ssh/authorized_keys' < ~/.ssh/id_rsa.pub


#########################################
changes for each node

NodeName public DNS = ec2-52-15-227-242.us-east-2.compute.amazonaws.com
- go to hadoop configuration folder
 - cd $HADOOP_CONF_DIR
 - sudo nano core-site.xml
	#####################
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://ec2-52-15-227-242.us-east-2.compute.amazonaws.com:9000</value>
  </property>
</configuration>
	#####################
- edit yarn-site.xml
	- nano yarn-site.xml
	#####################
<configuration>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>ec2-52-15-227-242.us-east-2.compute.amazonaws.com</value>
  </property>
</configuration>
	###################
- edit mapred-site.xml
	- nano mapred-site.xml
	###################
<configuration>
  <property>
    <name>mapreduce.jobtracker.address</name>
    <value>ec2-52-15-227-242.us-east-2.compute.amazonaws.com:54311</value>
  </property>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
  <property>
    <name>yarn.app.mapreduce.am.env</name>
    <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value>
  </property>
  <property>
    <name>mapreduce.map.env</name>
    <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value>
  </property>
  <property>
   <name>mapreduce.reduce.env</name>
   <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value>
  </property>
</configuration>
	###################

##########################################

Changes in NameNode
- change the host file
	- sudo nano /etc/hosts
	- go to every datanodes and run 
	- echo $(hostname)
	###################
	<namenode_IP> namenode_hostname
	<datanode1_IP> datanode1_hostname
	<datanode2_IP> datanode2_hostname
	<datanode3_IP> datanode3_hostname
	127.0.0.1 localhost
	###################
	namenode_IP = ip in ifconfig
	namenode_hostname = output of echo $(hostname)
- Edit hdfs-site.xml file on NameNode as below
	#####################
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>3</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:///usr/local/hadoop/data/hdfs/namenode</value>
  </property>
</configuration>
	#####################
- create namenode directory
	- sudo mkdir -p $HADOOP_HOME/data/hdfs/namenode
- Create masters file in HADOOP_CONF_DIR
	- nano masters
	- paste the hostname of namenode 
- edit the workers node
	- nano workers
	- paste datanode host name
	#################
	datanode1_hostname
	datanode2_hostname
	datanode3_hostname
	##################
- give ec2-user the permission for hadoop folder
	- sudo chown -R ec2-user $HADOOP_HOME


###########################################

Changes in DataNodes
- open  $HADOOP_CONF_DIR/hdfs-site.xml
- delete configration tag from the file and past below line into it

<configuration>
  <property>
    <name>dfs.replication</name>
    <value>3</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>file:///usr/local/hadoop/data/hdfs/datanode</value>
  </property>
</configuration>

- create datanode folder
	- sudo mkdir -p $HADOOP_HOME/data/hdfs/datanode
- give ec2-user the permission for hadoop folder
	- sudo chown -R ec2-user $HADOOP_HOME


#############################################
- hdfs namenode -format
start yarn manager, dfs and job tracker
- $HADOOP_HOME/sbin/start-all.sh
- to stop do $HADOOP_HOME/sbin/stop-all.sh

#############################################
Once Cluster is ready

1. Transfer states.tar.gz to NameNode using scp on your cluster
2. Extract it using tar -xvf states.tar.gz
Create the folder in HDFS(I  will create a HDFS folder for easy understanding) and transfer states folder into HDFS folder(a folder in hadoop filesystem)
3. hdfs dfs -mkdir -p HDFS
4. hdfs dfs -ls HDFS/
5. hdfs dfs -put 'states' HDFS
6 hdfs dfs -ls HDFS/states (check if all 50 states file are there)
Run jar file using below command(you can provide any jar files provided in jar folder)
7.1 hadoop jar 'CloudProject1.jar' HDFS/states HDFS/result (if you get error saying output path arealy exist then try HDFS/result1 or some different path )
7.2 hadoop jar 'CloudProject2.jar' HDFS/states HDFS/result (if you get error saying output path arealy exist then try HDFS/result1 or some different path )
To check the output 
8 hdfs dfs -cat HDFS/result/part-r-00000 (to check the result and subsequent parts to get the final result)

